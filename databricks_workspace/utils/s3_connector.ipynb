{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "940dfd44-bc3e-439f-8b5a-8dff0341112a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "\n",
    "class S3Connector:\n",
    "    def __init__(self, config_dict):\n",
    "        self._s3_bucket_name = config_dict[\"s3_bucket_name\"]\n",
    "        self._bucket_mount_name = config_dict[\"target_bucket_mount_name\"]\n",
    "        self._aws_credentials_filepath = config_dict[\"aws_authentication_csv_filepath\"]\n",
    "        self._is_bucket_mounted = self.check_if_bucket_already_mounted()\n",
    "\n",
    "    def check_if_bucket_already_mounted(self):\n",
    "        list_mounts = dbutils.fs.mounts()\n",
    "        mount_points = map(lambda x: x[0], list_mounts)\n",
    "        return self._bucket_mount_name in list(mount_points)\n",
    "\n",
    "    def __read_AWS_authentication_details_to_df(self):\n",
    "        file_type = \"csv\"\n",
    "        first_row_is_header = \"true\"\n",
    "        delimiter = \",\"\n",
    "        aws_keys_df = spark.read.format(file_type)\\\n",
    "                                .option(\"header\", first_row_is_header)\\\n",
    "                                .option(\"sep\", delimiter)\\\n",
    "                                .load(self._aws_credentials_filepath)\n",
    "                                \n",
    "        return aws_keys_df\n",
    "\n",
    "    def _extract_access_credentials(self):\n",
    "        \n",
    "        aws_keys_df = self.__read_AWS_authentication_details_to_df()\n",
    "\n",
    "        # Get the AWS access key and secret key from the spark dataframe\n",
    "        ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "        \n",
    "        SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "        \n",
    "        # Encode the secret key\n",
    "        ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "        \n",
    "        return (ACCESS_KEY, ENCODED_SECRET_KEY)\n",
    "\n",
    "    def mount_S3_bucket(self):\n",
    "\n",
    "        if self._is_bucket_mounted:\n",
    "            print(f\"Your S3 bucket has already been mounted under the name of {self._bucket_mount_name}\")\n",
    "        else:\n",
    "            aws_credentials_tuple = self._extract_access_credentials()\n",
    "\n",
    "            ACCESS_KEY = aws_credentials_tuple[0]\n",
    "            ENCODED_SECRET_KEY = aws_credentials_tuple[1]\n",
    "\n",
    "            SOURCE_URL = f\"s3n://{ACCESS_KEY}:{ENCODED_SECRET_KEY}@{self._s3_bucket_name}\"\n",
    "\n",
    "            dbutils.fs.mount(SOURCE_URL, self._bucket_mount_name)\n",
    "            self._is_bucket_mounted = True\n",
    "    \n",
    "    def unmount_S3_bucket(self):\n",
    "        dbutils.fs.unmount(self._bucket_mount_name)\n",
    "        self._is_bucket_mounted = False"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "s3_connector",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
