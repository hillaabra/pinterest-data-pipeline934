{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e797278-0785-4f96-b067-9d5743bf3171",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This script contains the dataframe cleaning methods defined for each dataset (to be used in\n",
    "# both the batch and the stream processing pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bdb10bb-9918-453f-9d88-311b3f59e1cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# necessary imports\n",
    "import re\n",
    "\n",
    "import pyspark.sql.dataframe\n",
    "from pyspark.sql.functions import array, col, concat_ws, to_timestamp, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a2daa9-e662-46c0-9a97-a71b53af14db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_follower_count_letters_to_digits(str: str) -> str:\n",
    "    \"\"\"\n",
    "    Callback function for a User Defined Function (follower_count_UDF, intended for use on\n",
    "    the follower_count column of the dataframe containing the Pinterest post data) which\n",
    "    converts letters 'M' and 'k' in a string to the number of zeros they represent in numeric\n",
    "    shorthand (i.e. 'M' stands for million ('000000') and 'k' stands for thousand ('000')).\n",
    "\n",
    "    Argument:\n",
    "    --------\n",
    "    str: str\n",
    "        The original alphanumeric string to be transformed.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    str: The transformed string of only numeric characters.\n",
    "    \"\"\"\n",
    "    if str is not None:\n",
    "        # normalising follower count column to contain only numeric characters\n",
    "        # (e.g. 15k --> 15000, 2M --> 2000000)\n",
    "        str = re.sub(r'M', '000000', str) \n",
    "        str = re.sub(r'k', '000', str)\n",
    "        return str\n",
    "    \n",
    "follower_count_UDF = udf(lambda x:convert_follower_count_letters_to_digits(x))\n",
    "\n",
    "def strip_down_to_save_location_filepath(str: str) -> str:\n",
    "    \"\"\"\n",
    "    Callback function for a User Defined Function (save_location_UDF, intended for use on\n",
    "    the save_location column of the dataframe containing the Pinterest post data) which\n",
    "    removes the phrase 'Local save in ' from the string, to leave just the filepath.\n",
    "    \n",
    "    Argument:\n",
    "    --------\n",
    "    str: str\n",
    "        The original string to be transformed.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    str: The transformed string of just the filepath.\n",
    "    \"\"\"\n",
    "    if str is not None:\n",
    "        # Normalising column so it just contains the filepath\n",
    "        str = re.sub(r'Local save in ', '', str)\n",
    "        return str\n",
    "    \n",
    "save_location_UDF = udf(lambda x:strip_down_to_save_location_filepath(x))\n",
    "\n",
    "def clean_pin_df(df_pin: pyspark.sql.dataframe) -> pyspark.sql.dataframe:\n",
    "    \"\"\"\n",
    "    Bespoke function designed to clean a Spark DataFrame containing the Pinterest post data.\n",
    "    To be used in both the Batch Layer and Stream Layer processing pipelines.\n",
    "\n",
    "    Argument:\n",
    "    --------\n",
    "    df_pin: pyspark.sql.dataframe\n",
    "        The uncleaned Spark DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pyspark.sql.dataframe: \n",
    "        The cleaned Spark DataFrame.\n",
    "    \"\"\"\n",
    "    # replace entries with no data or no useful data with None\n",
    "    dict_column_values_to_be_replaced = {\n",
    "        'follower_count': {\"User Info Error\": None},\n",
    "        'description': {\"No description available\": None, \"No description available Story format\": None},\n",
    "        'tag_list': {\"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\": None},\n",
    "        'title': {\"No Title Data Available\": None}, \n",
    "        'image_src': {\"Image src error.\": None}\n",
    "    }\n",
    "\n",
    "    for column in dict_column_values_to_be_replaced:\n",
    "        df_pin = df_pin.na.replace(dict_column_values_to_be_replaced[column], column)\n",
    "\n",
    "    # convert all follower_count entries to numeric values\n",
    "    df_pin = df_pin.withColumn(\"follower_count\", follower_count_UDF(col(\"follower_count\")).cast(IntegerType()))\n",
    "\n",
    "    # clean save_location column to include just filepath\n",
    "    df_pin = df_pin.withColumn(\"save_location\", save_location_UDF(col(\"save_location\")))\n",
    "\n",
    "    # rename index column to ind\n",
    "    df_pin = df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "    # reorder the columns (NB this drops the downloaded column)\n",
    "    df_pin = df_pin.select([\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\",\n",
    "                            \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\"])\n",
    "\n",
    "    return df_pin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d75dd74-b816-4aa9-98d2-82fe6454160f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_geo_df(df_geo: pyspark.sql.dataframe) -> pyspark.sql.dataframe:\n",
    "    \"\"\"\n",
    "    Bespoke function designed to clean a Spark DataFrame containing the geolocation data.\n",
    "    To be used in both the Batch Layer and Stream Layer processing pipelines.\n",
    "\n",
    "    Argument:\n",
    "    --------\n",
    "    df_pin: pyspark.sql.dataframe\n",
    "        The uncleaned Spark DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pyspark.sql.dataframe: \n",
    "        The cleaned Spark DataFrame.\n",
    "    \"\"\"\n",
    "    # combine the latitude and longitude columns into a single column 'coordinates'\n",
    "    df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "    \n",
    "    # convert the timestamp column from string to timestamp type\n",
    "    df_geo = df_geo.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy:MM:dd HH:mm:ss\"))\n",
    "\n",
    "    # reorder the columns and drop the longitude and latitude columns\n",
    "    df_geo = df_geo.select([\"ind\", \"country\", \"coordinates\", \"timestamp\"])\n",
    "\n",
    "    return df_geo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf5d9cb-e888-447a-9068-8330eeebe574",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_user_df(df_user: pyspark.sql.dataframe) -> pyspark.sql.dataframe:\n",
    "    \"\"\"\n",
    "    Bespoke function designed to clean a Spark DataFrame containing the user data.\n",
    "    To be used in both the Batch Layer and Stream Layer processing pipelines.\n",
    "\n",
    "    Argument:\n",
    "    --------\n",
    "    df_pin: pyspark.sql.dataframe\n",
    "        The uncleaned Spark DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pyspark.sql.dataframe: \n",
    "        The cleaned Spark DataFrame.\n",
    "    \"\"\"\n",
    "    # create a new column user_name that concatenates the information found in the first_name and last_name columns\n",
    "    df_user = df_user.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "    # convert date_joined column from string to timestamp data type\n",
    "    df_user = df_user.withColumn(\"date_joined\", to_timestamp(col(\"date_joined\"), \"yyyy:MM:dd HH:mm:ss\"))\n",
    "\n",
    "    # reorder the columns and drop the longitude and latitude columns, dropping first_name and last_name columns\n",
    "    df_user = df_user.select([\"ind\", \"user_name\", \"age\", \"date_joined\"]) \n",
    "\n",
    "    return df_user"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
