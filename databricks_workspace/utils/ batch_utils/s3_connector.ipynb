{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "940dfd44-bc3e-439f-8b5a-8dff0341112a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.dataframe\n",
    "import urllib\n",
    "\n",
    "\n",
    "class S3Connector:\n",
    "    \"\"\"\n",
    "    Class of methods required for mounting and unmounting the S3 bucket.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    config_dict: dict\n",
    "        A dictionary containing the keys \"s3_bucket_name\", \"target_bucket_mount_name\"\n",
    "        and \"aws_authentication_csv_filepath\" and their values.\n",
    "    \n",
    "    Attributes:\n",
    "    ----------\n",
    "    _s3_bucket_name: str\n",
    "        Protected; the name of the bucket in S3 to be mounted to DataBricks,\n",
    "        which holds the Batch Layer master data.\n",
    "    \n",
    "    _bucket_mount_name: str\n",
    "        Protected; the name to be given to the mount connecting Databricks to\n",
    "        the S3 bucket.\n",
    "    \n",
    "    _aws_credentials_filepath: str\n",
    "        Protected; the filepath to the CSV file in which the access tokens\n",
    "        required to connect with S3 are stored.\n",
    "    \n",
    "    _is_bucket_mounted: bool\n",
    "        Protected; set to True or False on initialisation by the\n",
    "        check_if_bucket_already_mounted() method.\n",
    "    \"\"\"\n",
    "    def __init__(self, config_dict: dict) -> None:\n",
    "        \"\"\"\n",
    "        See help(S3Connector) for an accurate signature.\n",
    "        \"\"\"\n",
    "        self._s3_bucket_name = config_dict[\"s3_bucket_name\"]\n",
    "        self._bucket_mount_name = config_dict[\"target_bucket_mount_name\"]\n",
    "        self._aws_credentials_filepath = config_dict[\"aws_authentication_csv_filepath\"]\n",
    "        self._is_bucket_mounted = self.check_if_bucket_already_mounted()\n",
    "\n",
    "    def check_if_bucket_already_mounted(self) -> bool:\n",
    "        \"\"\"\n",
    "        Method used to check if the bucket by the name stored at the object's\n",
    "        _bucket_mount_name attribute is already mounted.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        bool: True if the bucket mounting already exists, False otherwise.\n",
    "        \"\"\"\n",
    "        list_mounts = dbutils.fs.mounts()\n",
    "        mount_points = map(lambda x: x[0], list_mounts)\n",
    "        return self._bucket_mount_name in list(mount_points)\n",
    "\n",
    "    def __read_aws_authentication_details_to_df(self) -> pyspark.sql.dataframe:\n",
    "        file_type = \"csv\"\n",
    "        first_row_is_header = \"true\"\n",
    "        delimiter = \",\"\n",
    "        aws_keys_df = spark.read.format(file_type)\\\n",
    "                                .option(\"header\", first_row_is_header)\\\n",
    "                                .option(\"sep\", delimiter)\\\n",
    "                                .load(self._aws_credentials_filepath)\n",
    "                                \n",
    "        return aws_keys_df\n",
    "\n",
    "    def _extract_access_credentials(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Protected; method that extracts the security tokens from the\n",
    "        AWS credentials CSV file.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        tuple: (ACCESS_KEY, ENCODED_SECRET_KEY)\n",
    "        \"\"\"\n",
    "        aws_keys_df = self.__read_aws_authentication_details_to_df()\n",
    "\n",
    "        # Get the AWS access key and secret key from the spark dataframe\n",
    "        ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID'] \n",
    "        SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "\n",
    "        # Encode the secret key\n",
    "        ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "        \n",
    "        return (ACCESS_KEY, ENCODED_SECRET_KEY)\n",
    "\n",
    "    def mount_s3_bucket(self) -> None:\n",
    "        \"\"\"\n",
    "        Method which mounts the desired S3 bucket to Databricks, if it\n",
    "        is not already mounted.\n",
    "        \"\"\"\n",
    "        if self._is_bucket_mounted:\n",
    "            print(f\"Your S3 bucket has already been mounted under the name of {self._bucket_mount_name}\")\n",
    "        else:\n",
    "            aws_credentials_tuple = self._extract_access_credentials()\n",
    "\n",
    "            ACCESS_KEY = aws_credentials_tuple[0]\n",
    "            ENCODED_SECRET_KEY = aws_credentials_tuple[1]\n",
    "\n",
    "            SOURCE_URL = f\"s3n://{ACCESS_KEY}:{ENCODED_SECRET_KEY}@{self._s3_bucket_name}\"\n",
    "\n",
    "            dbutils.fs.mount(SOURCE_URL, self._bucket_mount_name)\n",
    "            self._is_bucket_mounted = True\n",
    "    \n",
    "    def unmount_s3_bucket(self) -> None:\n",
    "        \"\"\"\n",
    "        Method which unmounts the S3 bucket of the name assigned to the\n",
    "        object's _bucket_mount_name attribute.\n",
    "        \"\"\"\n",
    "        dbutils.fs.unmount(self._bucket_mount_name)\n",
    "        self._is_bucket_mounted = False"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "s3_connector",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
