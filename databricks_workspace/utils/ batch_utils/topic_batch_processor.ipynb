{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18bc1832-94f8-45bc-8b02-4b79b6efb1ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TopicBatchProcessor:\n",
    "    \"\"\"\n",
    "    Class which contains the ETL methods for the processing of the Batch Layer data.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    s3_connector_instance: S3Connector\n",
    "        An instance of the S3Connector class, providing connection to the S3 bucket on AWS. The bucket mount name is read from this object's properties to inform the _file_location attribute of the TopicBatchProcessor object.\n",
    "    \n",
    "    topic_name: str\n",
    "        The name of the topic to ingest the data from. This should match the naming of the Kafka topic created for this dataset in the Kafka (EC2) Client, and the subsequently named sub-directories in the S3 bucket that has been mounted.\n",
    "    \n",
    "    Attributes:\n",
    "    ----------\n",
    "    _topic_name: str\n",
    "        The name of the topic the data is being ingested from.\n",
    "    \n",
    "    _file_location: str\n",
    "        This is assigned on initialisation to reflect the pathway of the S3 folder being read from using the _bucket_mount_name attribute of the S3Connector object argument.\n",
    "    \n",
    "    _extracted_batch_data: NoneType or pyspark.sql.DataFrame\n",
    "        This is initialised as None; when data is read from the JSON files into a Spark DataFrame, a cache of that DataFrame is set to this attribute.\n",
    "\n",
    "    _cleaned_batch_data: NoneType or pyspark.sql.DataFrame \n",
    "        This is initialised as None; when the _extracted_batch_data DataFrame is cleaned, a cache of that DataFrame is set to this attribute.\n",
    "        \n",
    "    \"\"\"    \n",
    "    def __init__(self, s3_connector_instance, topic_name):\n",
    "        \"\"\"\n",
    "        See help(TopicBatchProcessor) for an accurate signature.\n",
    "        \"\"\"\n",
    "        self._topic_name = topic_name\n",
    "        self._file_location = f\"{s3_connector_instance._bucket_mount_name}/topics/{self._topic_name}/partition=0/*.json\"\n",
    "        self._extracted_batch_data = None\n",
    "        self._cleaned_batch_data = None \n",
    "    \n",
    "    def read_json_files_from_s3_bucket_to_df(self) -> None:\n",
    "        \"\"\"\n",
    "        Method that reads the JSON files from the relevant topic subfolder of mounted S3 bucket to a Spark DataFrame that is cached to the object's _extracted_batch_data attribute.\n",
    "        \"\"\"\n",
    "        file_type = \"json\"\n",
    "        infer_schema = \"true\"\n",
    "        df = spark.read.format(file_type) \\\n",
    "                        .option(\"inferSchema\", infer_schema) \\\n",
    "                        .load(self._file_location)\n",
    "\n",
    "        self._extracted_batch_data = df\n",
    "    \n",
    "    def clean_dataframe(self, cleaning_function) -> None:\n",
    "        \"\"\"\n",
    "        Method that cleans the DataFrame assigned to the _extracted_batch_data attribute using a bespoke cleaning function, and caches the cleaned DataFrame to the object's _cleaned_batch_data attribute. \n",
    "\n",
    "        Argument:\n",
    "        --------\n",
    "        cleaning_function: Callable\n",
    "            A function defining how the dataframe of extracted data should be cleaned in the case of this dataset (and returns the cleaned dataframe).\n",
    "        \"\"\"\n",
    "        if self._extracted_batch_data is not None:\n",
    "            try:\n",
    "                cleaned_df = cleaning_function(self._extracted_batch_data)\n",
    "                self._cleaned_batch_data = cleaned_df.cache()\n",
    "            except Exception as e:\n",
    "                print(f\"Error occured during cleaning of {self._topic_name} extracted data: {e}\")\n",
    "        else:\n",
    "            print(f\"Error occured during cleaning of {self._topic_name} extracted data:\\\n",
    "                    no dataframe of extracted data saved to object instance.\")     "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "topic_batch_processor",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
