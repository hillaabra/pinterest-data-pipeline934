{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a13d41-808c-4921-9188-0131a018a32e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Users/hilla.abramov@gmail.com/utils/stream_utils/extract_aws_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b81e5575-a44e-4687-b1dc-679ca21536c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections.abc import Callable # for type-hinting\n",
    "from pyspark.sql.types import StructType # for type-hinting\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "\n",
    "class StreamProcessor:\n",
    "    \"\"\"\n",
    "    Class containing ETL methods to be used on a data stream within the Stream Layer\n",
    "    of the pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    kinesis_stream_name: str\n",
    "        The name of the Kinesis data stream to read data from. This should match\n",
    "        the naming of the data streams created for this dataset on the AWS Kinesis console.\n",
    "\n",
    "    delta_table_name: str\n",
    "        The name to give the delta table the processed stream data should be appended to.\n",
    "\n",
    "    json_schema: pyspark.sql.types.Structype\n",
    "        The StructType JSON input schema to define typing of new stream data after ingestion.\n",
    "    \n",
    "    cleaning_function: Callable\n",
    "        The function defining how the data should be cleaned/transformed before being stored.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    _kinesis_stream_name: str\n",
    "        Protected; the name of the Kinesis data stream to read data from.\n",
    "        \n",
    "    _delta_table_name: str\n",
    "        Protected; the name of the Delta Table the processed stream data is appended to.\n",
    "    \n",
    "    _json_schema: pyspark.sql.types.StructType\n",
    "        Protected; the StructType JSON input type schema according to which the new stream data is mapped immediately after ingestion.\n",
    "        \n",
    "    _input_streaming_df: NoneType or pyspark.sql.DataFrame\n",
    "        Protected; initialised as None; assigned pyspark.sql.DataFrame when reading of stream begins.\n",
    "        \n",
    "    _cleaned_streaming_df: NoneType or pyspark.sql.DataFrame\n",
    "        Protected; initialised as None; assigned pyspark.sql.DataFrame when cleaning of stream begins.\n",
    "    \n",
    "    _cleaning_function: Callable\n",
    "        Protected; the function defining how the data should be cleaned/transformed before being stored.\n",
    "    \"\"\"\n",
    "    def __init__(self, kinesis_stream_name: str, delta_table_name: str, json_schema: StructType, cleaning_function: Callable):\n",
    "        \"\"\"\n",
    "        See help(StreamProcessor) for an accurate signature.\n",
    "        \"\"\"\n",
    "        self._kinesis_stream_name = kinesis_stream_name\n",
    "        self._delta_table_name = delta_table_name\n",
    "        self._json_schema = json_schema\n",
    "        self._input_streaming_df = None\n",
    "        self._cleaned_streaming_df = None\n",
    "        self._cleaning_function = cleaning_function\n",
    "\n",
    "    def _read_stream_from_kinesis_to_spark_df(self) -> None:\n",
    "        \"\"\"\n",
    "        Protected; method which initialises the structured streaming within Spark DataFrames, assigning a stream DataFrame to the object's _input_streaming_df atttribute, to which it reads the incoming data from Kinesis. Applies the input JSON schema stored at the object's _json_schema attribute to incoming data.\n",
    "        \"\"\"\n",
    "        self._input_streaming_df = spark.readStream \\\n",
    "                                        .format('kinesis') \\\n",
    "                                        .option('streamName', self._kinesis_stream_name) \\\n",
    "                                        .option('initialPosition','earliest') \\\n",
    "                                        .option('region','us-east-1') \\\n",
    "                                        .option('awsAccessKey', ACCESS_KEY) \\\n",
    "                                        .option('awsSecretKey', SECRET_KEY) \\\n",
    "                                        .load() \\\n",
    "                                        .selectExpr(\"cast (data as STRING) jsonData\") \\\n",
    "                                        .select(from_json(\"jsonData\", self._json_schema).alias(self._kinesis_stream_name)) \\\n",
    "                                        .select(f\"{self._kinesis_stream_name}.*\")\n",
    "\n",
    "    def _clean_stream_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Protected; method which applies the predefined cleaning function to the incoming stream of data, and saving the cleaned data to a pyspark.sql.DataFrame at the object's _cleaned_streaming_df attribute. \n",
    "        \"\"\"\n",
    "        self._cleaned_streaming_df = self._cleaning_function(self._input_streaming_df)\n",
    "    \n",
    "    def _write_stream_to_delta_table(self) -> None:\n",
    "        \"\"\"\n",
    "        Protected; method which appends the cleaned stream data to its assigned Delta table.\n",
    "        \"\"\"\n",
    "        checkpoint_location = f\"/tmp/delta/{self._delta_table_name}/_checkpoints/\"\n",
    "        \n",
    "        # first remove checkpoint location if already exists\n",
    "        if checkpoint_location in dbutils.fs.ls(\"/tmp\"):\n",
    "            dbutils.fs.rm(checkpoint_location, True)\n",
    "\n",
    "        # write stream to delta table\n",
    "        self._cleaned_streaming_df.writeStream \\\n",
    "            .format(\"delta\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "            .table(self._delta_table_name)\n",
    "    \n",
    "    def run_pipeline(self) -> None:\n",
    "        \"\"\"\n",
    "        Method which runs the full ETL pipeline for the stream (reading from the data stream in Kinesis, cleaning the data and writing it to its Delta table).\n",
    "        \"\"\"\n",
    "        self._read_stream_from_kinesis_to_spark_df()\n",
    "        self._clean_stream_data()\n",
    "        self._write_stream_to_delta_table()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "kinesis_stream_processor",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
